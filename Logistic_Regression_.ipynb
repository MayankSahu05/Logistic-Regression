{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Logistic Regression\n",
        "\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "- Logistic Regression is a supervised classification algorithm used to predict a binary (or multiclass via extensions) outcome. It models the probability that a given input belongs to a class using the logistic (sigmoid) function. Unlike Linear Regression which predicts continuous numeric values using a linear equation Y = b0 + b1*X + ..., Logistic Regression predicts probabilities between 0 and 1 and then thresholds them to produce class labels. The model output is p = sigmoid(z) where z = b0 + b1*X + ....\n",
        "\n",
        "2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "-  The sigmoid (logistic) function maps any real-valued number z into the range (0, 1):  \n",
        "         sigmoid(z) = 1 / (1 + exp(-z))\n",
        "In Logistic Regression, the linear combination z = b0 + b1*x1 + ... is passed through sigmoid to produce a probability p that the observation belongs to class 1. This probability is used for classification (e.g., classify as 1 if p >= 0.5) and for likelihood-based training.\n",
        "\n",
        "3. What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "-  Regularization adds a penalty term to the loss function to limit model complexity and avoid overfitting. Common types:\n",
        "\n",
        " - L2 (Ridge): penalty = lambda * sum(coef^2)\n",
        "\n",
        " - L1 (Lasso): penalty = lambda * sum(|coef|)\n",
        "  # Why needed:\n",
        "\n",
        "-  Reduces overfitting on training data.\n",
        "\n",
        " - L1 can perform feature selection (drives some coefficients to zero).\n",
        "\n",
        " - L2 shrinks coefficients to stabilize model and improve generalization.\n",
        "\n",
        "4.  What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "Answer:\n",
        "\n",
        " - Accuracy: fraction of correct predictions. Good for balanced classes.\n",
        "\n",
        " - Precision: TP / (TP + FP) — important when false positives cost more.\n",
        "\n",
        " - Recall (Sensitivity): TP / (TP + FN) — important when false negatives cost more.\n",
        "\n",
        " - F1-score: harmonic mean of precision and recall — good for imbalanced classes.\n",
        "\n",
        " - ROC-AUC: area under ROC curve — measures discrimination ability across thresholds.\n",
        "\n",
        " - Confusion Matrix: breakdown of TP, FP, TN, FN — helps understand types of errors.\n",
        "These metrics help evaluate model performance depending on business priorities and class balance.\n",
        "\n",
        "5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "hpNXaXqrs8Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Load CSV-like dataset (sklearn), split, train logistic regression, print accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQSMCPhvYkj",
        "outputId": "b8d12174-36bb-43c1-c7aa-027ffe939541"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. : Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "hpfnBGzAvgaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. L2 regularization (Ridge) example using breast_cancer dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# C is inverse of regularization strength. default penalty='l2'\n",
        "model_l2 = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=10000)\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_l2.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy (L2):\", acc)\n",
        "print(\"Coefficients:\", model_l2.coef_)\n",
        "print(\"Intercept:\", model_l2.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSHvPrVevrDg",
        "outputId": "17063fbb-fc5b-4284-bb5a-7900c2f1dee1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (L2): 0.956140350877193\n",
            "Coefficients: [[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            "  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            "  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n",
            "  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n",
            "  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n",
            "   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n",
            "  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n",
            "  -7.42763610e-01 -1.16960181e-01]]\n",
            "Intercept: [0.40847797]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "G0v2lLXWxCax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Multiclass classification with multi_class='ovr' on iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use multi_class='ovr'\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=10000)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_ovr.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1DPa5_JxS5Q",
        "outputId": "723ec2ce-d728-412e-a513-1ed11102e74a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "nrYQSDxexpyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. GridSearchCV to tune C and penalty\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter grid: try l1 and l2 penalties with solver 'liblinear' (binary only)\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(solver='liblinear', max_iter=10000)\n",
        "\n",
        "grid = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV score:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Accuracy with best params:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn9OB42XyB1S",
        "outputId": "c1923ef7-cf8a-4695-9bee-21aa6c31394d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'C': 100, 'penalty': 'l1'}\n",
            "Best CV score: 0.9670329670329672\n",
            "Test Accuracy with best params: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "F7M7-wajyNlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Compare accuracy with and without StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model_raw = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy without scaling:\", acc_raw)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So4vCrRWyYvE",
        "outputId": "22522c01-3039-4444-9a67-12e532f90cae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case"
      ],
      "metadata": {
        "id": "dSL2chfqygNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Answer (practical pipeline):\n",
        "\n",
        " 1. Understand business & metric:\n",
        "\n",
        " - Determine cost of false positives vs false negatives. Use metrics like Precision, Recall, F1, PR-AUC, and business KPIs (e.g., campaign ROI).\n",
        "\n",
        "  2. Data preprocessing & features:\n",
        "\n",
        " - Clean data, handle missing values, create meaningful features (recency, frequency, monetary, engagement features).\n",
        "\n",
        " - Convert categorical features with one-hot or target encoding (careful with target leakage).\n",
        "\n",
        "  3. Train-test split:\n",
        "\n",
        " - Use stratified split to maintain class ratio in train/test sets.\n",
        "\n",
        " 4. Feature scaling & pipelines:\n",
        "\n",
        " - Use StandardScaler or RobustScaler in a pipeline along with model to avoid data leakage.\n",
        "\n",
        "5. Handle class imbalance:\n",
        "\n",
        " - Try sampling methods: oversampling minority (SMOTE), undersampling majority, or combined.\n",
        "\n",
        " - Alternatively use class-weighted loss: LogisticRegression(class_weight='balanced') or set custom weights to penalize misclassification of minority more.\n",
        "\n",
        " - Prefer cross-validated sampling or use pipelines so sampling is only applied to training fold.\n",
        "\n",
        "  6. Model selection & regularization:\n",
        "\n",
        " - Use regularized logistic regression (L1/L2) and tune C (inverse regularization). L1 for feature selection.\n",
        "\n",
        " - Consider"
      ],
      "metadata": {
        "id": "Fz6GtkhKy6aD"
      }
    }
  ]
}